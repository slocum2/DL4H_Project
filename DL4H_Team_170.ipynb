{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfk8Zrul_E8V"
   },
   "source": [
    "**Deep Learning For Healthcare - Final Project**\n",
    "\n",
    "**Team 170 (Noah Slocum)**\n",
    "\n",
    "**GitHub Repo: https://github.com/slocum2/DL4H_Project**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### Paper citation\n",
    "Feng, YH., Zhang, SW. & Shi, JY. DPDDI: a deep predictor for drug-drug interactions. BMC Bioinformatics 21, 419 (2020). https://doi.org/10.1186/s12859-020-03724-x\n",
    "\n",
    "### Background\n",
    "\n",
    "Understanding drug-drug interactions (DDIs) is important to developing any new medication or combinatorial prescription. Medications are frequently prescribed together for desired synergistic effects that may be greater than the effect of either medication alone. On the contrary, taking two medications simultaneously can also be dangerous if they interact to produce undesired or even toxic effects. Traditionally, drug-drug interactions have been investigated and identified in a wet lab, which is costly and very time consuming. This is true whether in vitro or in vivo methods are used. Considering the continuously growing number of drugs available for prescription, this task is only becoming more difficult as time goes on. \n",
    "\n",
    "In contrast to wet-lab methods, computational methods have been used to discover DDIs in two ways: one, by finding annotated DDIs in literature, medical records, and other forms of unstructured text, and two, by predicting DDIs using machine learning and detailed descriptions of drug properties. The former is useful but cannot predict DDIs before combinatorial treatment, while the latter requires that machine learning features be carefully crafted from detailed drug characteristic data (including chemical structure, targets, ATC codes, side effects, and clinical observations). \n",
    "\n",
    "While the latter method is useful in many cases, detailed data about drug properties may not always be available. Drugs without complete data may not be included in DDI prediction models and thus the model may not be pragmatic enough for real scenarios where the ignored drugs can still be combinatorially prescribed. The general problem Feng et al attempt to solve is that of enhancing DDI prediction to eliminate the need for hand-crafted features and to avoid ignoring or removing drugs with incomplete data.\n",
    "\n",
    "In this paper, Feng et al describe a 2-fold approach to solving the problem stated above. First, they use a graph convolution network (GCN) to learn the low-dimensional feature representation of each drug. Using a graph data structure along with a convolutional network captures the topological relationship of each drug to neighboring drugs. This approach is unsupervised and doesn’t require manual feature engineering or complete data for each drug. The second step is feeding the low-dimensional feature representation of each drug (created by the GCN) to a deep neural network (DNN) for DDI prediction. This is done by aggregating features representing two drugs into one feature that represents the DDI.\n",
    "\n",
    "### Hypothesis\n",
    "The hypothesis is that the specific approach described above will perform better than state-of-art DDI predictors. This hypothesis is based on the following ideas:\n",
    "\n",
    "1. The deep learning features created by a GCN will outperform other features hand-crafted from chemical, biological or anatomical properties of drugs.\n",
    "2. The GCN-created features can be aggregated to represent DDIs and then used to train a deep neural network for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import copy\n",
    "import csv\n",
    "import h5py\n",
    "from keras import *\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "import tensorflow.compat.v1 as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "#  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "filename = 'DATA.mat'\n",
    "data = h5py.File(filename,'r')\n",
    "adj = data['Adj_binary'][:]\n",
    "adj = adj.transpose()\n",
    "adj_copy = copy.deepcopy(adj)\n",
    "adj = sp.csr.csr_matrix(adj)\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis,:], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "# calculate statistics\n",
    "def calculate_stats(raw_data):\n",
    "  # TO DO\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "#   Model\n",
    "\n",
    "The following is a diagram representing the DPDDI model:\n",
    "\n",
    "![dpddi](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12859-020-03724-x/MediaObjects/12859_2020_3724_Fig2_HTML.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Auto Encoder (GAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "def weight_variable_glorot1(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010) initialization.\n",
    "    \"\"\"\n",
    "#    tf.set_random_seed(123)\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
    "                                maxval=init_range, dtype=tf.float32,seed = 12)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_glorot2(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010) initialization.\n",
    "    \"\"\"\n",
    "#    tf.set_random_seed(231)\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
    "                                maxval=init_range, dtype=tf.float32,seed = 50)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs\n",
    "    \"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.issparse = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            outputs = self._call(inputs)\n",
    "            return outputs\n",
    "\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, dropout=0., act=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot1(input_dim, output_dim, name=\"weights\")\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        x = tf.matmul(x, self.vars['weights'])\n",
    "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "        outputs = self.act(x)\n",
    "        self.w2 = self.vars['weights']\n",
    "        return outputs,self.w2\n",
    "\n",
    "class DeepConvolution(Layer):\n",
    "    \"\"\"Basic deep  layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout=0., act=tf.nn.relu, **kwargs):\n",
    "        super(DeepConvolution, self).__init__(**kwargs)\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot2(input_dim, output_dim, name=\"weights2\")\n",
    "        self.dropout = 0.0001\n",
    "        self.act = act\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        x = tf.matmul(x, self.vars['weights'])\n",
    "        outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparse(Layer):\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\n",
    "        super(GraphConvolutionSparse, self).__init__(**kwargs)\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot2(input_dim, output_dim, name=\"weights\")\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.features_nonzero = features_nonzero\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
    "        x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
    "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "        outputs = self.act(x)\n",
    "        w1 = self.vars['weights']\n",
    "        return outputs,w1\n",
    "\n",
    "\n",
    "class InnerProductDecoder(Layer):\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
    "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "        x = tf.transpose(inputs)\n",
    "        x = tf.matmul(inputs, x)\n",
    "        x = tf.reshape(x, [-1])\n",
    "        outputs = self.act(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class GCNModelAE(Model):\n",
    "    def __init__(self, placeholders, num_features, features_nonzero, **kwargs):\n",
    "        super(GCNModelAE, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.adj = placeholders['adj']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.hidden1,self.w1 = GraphConvolutionSparse(input_dim=self.input_dim,\n",
    "                                              output_dim=FLAGS.hidden1,\n",
    "                                              adj=self.adj,\n",
    "                                              features_nonzero=self.features_nonzero,\n",
    "                                              act=tf.nn.relu,\n",
    "                                              dropout=self.dropout,\n",
    "                                              logging=self.logging)(self.inputs)\n",
    "\n",
    "        self.embeddings,self.w2 = GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                           output_dim=FLAGS.hidden2,\n",
    "                                           adj=self.adj,\n",
    "                                           act=lambda x: x,\n",
    "                                           dropout=self.dropout,\n",
    "                                           logging=self.logging)(self.hidden1)\n",
    "#        self.embeddings1 = DeepConvolution(input_dim=FLAGS.hidden2,\n",
    "#                                           output_dim=FLAGS.hidden3,\n",
    "#                                           \n",
    "#                                           act=lambda x: x,\n",
    "#                                           dropout=0.0001,\n",
    "#                                           logging=self.logging)(self.embeddings)\n",
    "#        self.embeddings2 = DeepConvolution(input_dim=FLAGS.hidden3,\n",
    "#                                           output_dim=FLAGS.hidden4,\n",
    "#                                           \n",
    "#                                           act=lambda x: x,\n",
    "#                                           dropout=self.dropout,\n",
    "#                                           logging=self.logging)(self.embeddings1)   \n",
    "\n",
    "\n",
    "        self.z_mean = self.embeddings\n",
    "#        self.z_mean = self.embeddings1\n",
    "#        self.z_mean = self.embeddings2\n",
    "\n",
    "        self.reconstructions = InnerProductDecoder(input_dim=FLAGS.hidden2,\n",
    "                                      act=lambda x: x,\n",
    "                                      logging=self.logging)(self.embeddings)\n",
    "\n",
    "\n",
    "class GCNModelVAE(Model):\n",
    "    def __init__(self, placeholders, num_features, num_nodes, features_nonzero, **kwargs):\n",
    "        super(GCNModelVAE, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.n_samples = num_nodes\n",
    "        self.adj = placeholders['adj']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim,\n",
    "                                              output_dim=FLAGS.hidden1,\n",
    "                                              adj=self.adj,\n",
    "                                              features_nonzero=self.features_nonzero,\n",
    "                                              act=tf.nn.relu,\n",
    "                                              dropout=self.dropout,\n",
    "                                              logging=self.logging)(self.inputs)\n",
    "\n",
    "        self.z_mean = GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                       output_dim=FLAGS.hidden2,\n",
    "                                       adj=self.adj,\n",
    "                                       act=lambda x: x,\n",
    "                                       dropout=self.dropout,\n",
    "                                       logging=self.logging)(self.hidden1)\n",
    "\n",
    "        self.z_log_std = GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                          output_dim=FLAGS.hidden2,\n",
    "                                          adj=self.adj,\n",
    "                                          act=lambda x: x,\n",
    "                                          dropout=self.dropout,\n",
    "                                          logging=self.logging)(self.hidden1)\n",
    "\n",
    "        self.z = self.z_mean + tf.random_normal([self.n_samples, FLAGS.hidden2]) * tf.exp(self.z_log_std)\n",
    "\n",
    "        self.reconstructions = InnerProductDecoder(input_dim=FLAGS.hidden2,\n",
    "                                      act=lambda x: x,\n",
    "                                      logging=self.logging)(self.z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerAE(object):\n",
    "    def __init__(self, preds, labels, pos_weight, norm):#,w1,w2\n",
    "        preds_sub = preds\n",
    "        labels_sub = labels\n",
    "#        tf.add_to_collection(tf.GraphKeys.WEIGHTS, w1)\n",
    "#        tf.add_to_collection(tf.GraphKeys.WEIGHTS, w2)\n",
    "#        regularizer = tf.contrib.layers.l2_regularizer(scale=5.0/1000)\n",
    "#        reg_term = tf.contrib.layers.apply_regularization(regularizer)\n",
    "\n",
    "        self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub,\\\n",
    "                          targets=labels_sub, pos_weight=pos_weight))   #  +reg_term\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)  # Adam Optimizer\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
    "\n",
    "        self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n",
    "                                           tf.cast(labels_sub, tf.int32))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "class OptimizerVAE(object):\n",
    "    def __init__(self, preds, labels, model, num_nodes, pos_weight, norm):  #,w1,w2\n",
    "        preds_sub = preds\n",
    "        labels_sub = labels\n",
    "\n",
    "        self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)  # Adam Optimizer\n",
    "\n",
    "        # Latent loss\n",
    "        self.log_lik = self.cost\n",
    "        self.kl = (0.5 / num_nodes) * tf.reduce_mean(tf.reduce_sum(1 + 2 * model.z_log_std - tf.square(model.z_mean) -\n",
    "                                                                   tf.square(tf.exp(model.z_log_std)), 1))\n",
    "        self.cost -= self.kl\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
    "\n",
    "        self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n",
    "                                           tf.cast(labels_sub, tf.int32))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPDDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPDDI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_score1(edges_pos, edges_neg,emb=None):\n",
    "    if emb is None:\n",
    "        feed_dict.update({placeholders['dropout']: 0})\n",
    "        emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pred_probability_pos = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        pred_probability_pos.append(adj_rec[e[0], e[1]])\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    pred_probability_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        pred_probability_neg.append(adj_rec[e[0], e[1]])\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_probability_all = np.hstack([pred_probability_pos, pred_probability_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_probability_all)   ## preds_all\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(labels_all, preds_probability_all)\n",
    "    aupr_score = auc(recall, precision)   \n",
    "\n",
    "    return roc_score, aupr_score\n",
    "\n",
    "def get_roc_score2(pre,preds_probability_all,y_test):\n",
    "   \n",
    "    preds_all = np.array(pre)\n",
    "    preds_probability_all = np.array(pre)\n",
    "    labels_all = np.array(y_test)\n",
    "    roc_score = roc_auc_score(labels_all,preds_probability_all)   \n",
    "    \n",
    "    precision, recall, pr_thresholds = precision_recall_curve(labels_all, preds_probability_all )  ##preds_all   preds_probability_all\n",
    "    aupr_score = auc(recall, precision)\n",
    "#    \n",
    "    all_F_measure=np.zeros(len(pr_thresholds))\n",
    "    for k in range(0,len(pr_thresholds)):\n",
    "        if (precision[k]+precision[k])>0:\n",
    "            all_F_measure[k]=2*precision[k]*recall[k]/(precision[k]+recall[k])\n",
    "        else:\n",
    "            all_F_measure[k]=0\n",
    "    max_index=all_F_measure.argmax()\n",
    "    threshold=pr_thresholds[max_index]\n",
    "#        \n",
    "    fpr, tpr, auc_thresholds = roc_curve(labels_all, preds_probability_all)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    predicted_score=np.zeros(shape=(len(labels_all),1))\n",
    "    predicted_score[preds_probability_all>threshold]=1\n",
    "    confusion_matri = confusion_matrix(y_true=labels_all, y_pred=predicted_score)\n",
    "    print(\"confusion_matrix:\",confusion_matri)\n",
    "        \n",
    "    f=f1_score(labels_all,predicted_score)\n",
    "    accuracy=accuracy_score(labels_all,predicted_score)\n",
    "    precision=precision_score(labels_all,predicted_score)\n",
    "    recall=recall_score(labels_all,predicted_score)\n",
    "\n",
    "    return roc_score, aupr_score,precision, recall,accuracy,f\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
    "    # construct feed dictionary\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
    "    feed_dict.update({placeholders['adj_orig']: adj})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "    \n",
    "def mask_test_edges(adj):\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = [],[],[],[],[],[]\n",
    "    adj_train = []\n",
    "#    kfold = 5\n",
    "#    num_train_kd = edges.shape[0]// kfold\n",
    "#    num_test_kd = int(np.floor(num_train_kd * 0.2))\n",
    "#    num_val_kd = int(np.floor(num_train_kd * 0.5))   #num_train_kd / 20.\n",
    "\n",
    "    link_number = 0\n",
    "    non_link_number = 0\n",
    "    link_position = []\n",
    "    non_link_position = []  # all non-link position\n",
    "    for i in range(0, adj.shape[0]):\n",
    "        for j in range(i + 1, adj.shape[1]):\n",
    "            if adj[i, j] == 1:\n",
    "                link_number = link_number + 1\n",
    "                link_position.append([i, j])\n",
    "            elif adj[i,j] ==0:\n",
    "                non_link_number = non_link_number +1\n",
    "                non_link_position.append([i,j])\n",
    "\n",
    "\n",
    "    link_position = np.array(link_position)\n",
    "    non_link_position = np.array(non_link_position)\n",
    "    seed = 12\n",
    "    random.seed(seed)\n",
    "    link_index = np.arange(0, link_number)\n",
    "    non_link_index = np.arange(0,non_link_number)\n",
    "    random.shuffle(link_index)\n",
    "    random.shuffle(non_link_index)\n",
    "    kfold = 5\n",
    "    num_train_kd = link_number// kfold\n",
    "    num_test_kd = int(np.floor(num_train_kd * 0.2))\n",
    "    num_val_kd = int(np.floor(num_train_kd * 0.5))   #num_train_kd / 20.\n",
    "    num_negtive_kd = non_link_number // kfold\n",
    "    \n",
    "    for i in range(kdfold):\n",
    "        train_link_index = link_index[i *num_train_kd:(i+1)*num_train_kd]        \n",
    "        test_link_index = train_link_index[0:num_test_kd]\n",
    "        val_link_index = train_link_index[num_test_kd:num_test_kd + num_val_kd]\n",
    "        train_index = train_link_index[num_test_kd + num_val_kd:num_train_kd]\n",
    "        \n",
    "        train_index.sort()\n",
    "        val_link_index.sort()\n",
    "        test_link_index.sort()       \n",
    "        train_edges.append(link_position[train_index])\n",
    "        val_edges.append(link_position[val_link_index])\n",
    "        test_edges.append(link_position[test_link_index])\n",
    "        \n",
    "        fold =6\n",
    "        kd_no_link_index = non_link_index[i*num_negtive_kd:(i+1)*num_negtive_kd]\n",
    "        test_no_link_index = kd_no_link_index[0: fold * num_test_kd]\n",
    "        val_no_link_index = kd_no_link_index[fold * num_test_kd:fold * (num_test_kd + num_val_kd)]\n",
    "        train_no_link_index = kd_no_link_index[fold * (num_test_kd + num_val_kd):num_negtive_kd]\n",
    "        \n",
    "        train_no_index.sort()\n",
    "        val_no_link_index.sort()\n",
    "        test_no_link_index.sort()       \n",
    "        train_edges_false.append(non_link_position[train_no_index])\n",
    "        val_edges_false.append(non_link_position[val_no_link_index])\n",
    "        test_edges_false.append(non_link_position[test_no_link_index])\n",
    "        data = np.ones(train_edges.shape[0])\n",
    "        # Re-build adj matrix\n",
    "        adj_train_rebuild = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "        adj_train_rebuild = adj_train_rebuild + adj_train_rebuild.T\n",
    "        adj_train.append(adj_train_rebuild)\n",
    "    \n",
    "    return adj_train, train_edges, train_edges_false,val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "def node_trans_edges(test_edges,test_edges_false,val_edges,val_edges_false,train_edges,train_edges_false):\n",
    "    if type(test_edges) != list:\n",
    "        test_edges = test_edges.tolist()\n",
    "    if type(train_edges_false) != list:\n",
    "        train_edges_false = train_edges_false.tolist()\n",
    "    if type(train_edges) != list:\n",
    "        train_edges = train_edges.tolist()\n",
    "    if type(test_edges_false) != list:\n",
    "        test_edges_false = test_edges_false.tolist()  \n",
    "    if type(val_edges) != list:\n",
    "        val_edges = val_edges.tolist() \n",
    "    if type(val_edges_false) != list:\n",
    "        val_edges_false = val_edges_false.tolist() \n",
    "        \n",
    "    x_train_index = train_edges + train_edges_false\n",
    "    y_train = [1]*len(train_edges) + [0] * len(train_edges_false)\n",
    "    x_val_index = val_edges + val_edges_false\n",
    "    y_val = [1]*len(val_edges) + [0] * len(val_edges_false)\n",
    "    x_test_index = test_edges + test_edges_false\n",
    "    y_test = [1]*len(test_edges) + [0] * len(test_edges_false)\n",
    "    x_train = []\n",
    "    x_val = []\n",
    "    x_test = []\n",
    "    ###transform node embdding to edges feature by concat opration \n",
    "    t = time.time()\n",
    "    for i in range(len(x_train_index)):\n",
    "        x_train.append(np.hstack((embedding[ x_train_index[i][0]],embedding[ x_train_index[i][1]])))\n",
    "    for i in range(len(x_val_index)):\n",
    "        x_val.append(np.hstack((embedding[ x_val_index[i][0]],embedding[ x_val_index[i][1]])))\n",
    "    for i in range(len(x_test_index)):\n",
    "        x_test.append(np.hstack((embedding[ x_test_index[i][0]] ,embedding[ x_test_index[i][1]])))\n",
    "    print(\"cost time of embedding concat\", time.time()-t)\n",
    "    \n",
    "    y_train = utils.to_categorical(y_train, 2)\n",
    "    y_test = utils.to_categorical(y_test, 2)\n",
    "    y_val =  utils.to_categorical(y_val, 2)\n",
    "    \n",
    "    x_train = np.matrix(x_train)\n",
    "    x_test = np.matrix(x_test)\n",
    "    x_val = np.matrix(x_val)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    y_val = np.array(y_val)\n",
    "    print(\"fill embedding data accomplishment\")\n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPDDI Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.x_val,self.y_val = validation_data\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print('\\n ROC_AUC - epoch:%d - score:%.6f \\n' % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPDDI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "flags = tf.compat.v1.flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 1200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 700, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 256, 'Number of units in hidden layer 2. ')\n",
    "#flags.DEFINE_integer('hidden3', 128, 'Number of units in hidden layer 2. ')\n",
    "#flags.DEFINE_integer('hidden4', 128, 'Number of units in hidden layer 2. ')\n",
    "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "\n",
    "flags.DEFINE_string('model', 'gcn_ae', 'Model string.')\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')\n",
    "flags.DEFINE_integer('features',0, 'Whether to use features (1) or not (0).')\n",
    "\n",
    "model_str = 'gcn_ae'\n",
    "\n",
    "filename = 'DATA.mat'\n",
    "data = h5py.File(filename,'r')\n",
    "adj = data['Adj_binary'][:]\n",
    "adj = adj.transpose()\n",
    "adj_copy = copy.deepcopy(adj)\n",
    "adj = sp.csr.csr_matrix(adj)\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis,:], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "if FLAGS.features == 0:\n",
    "    features = sp.identity(adj.shape[0])  # featureless\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=())\n",
    "}\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "roc_score_arr,aupr_score_arr,precision_arr, recall_arr,accuracy_arr,f_arr = [],[],[],[],[],[]\n",
    "print(\"split end\")\n",
    "CV = 5\n",
    "for i in range(CV):\n",
    "    adj = adj_train[i]\n",
    "    adj_norm = preprocess_graph(adj)   # Some preprocessing\n",
    "    model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    with tf.name_scope('optimizer'):      # Optimizer\n",
    "        if model_str == 'gcn_ae':\n",
    "            opt = OptimizerAE(preds=model.reconstructions,\n",
    "                          labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                      validate_indices=False), [-1]),\n",
    "                          pos_weight=pos_weight,\n",
    "                          norm=norm)\n",
    "        \n",
    "    sess = tf.Session()    # Initialize session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_val = []\n",
    "    acc_val = []\n",
    "    val_roc_score = []\n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    adj_label = sparse_to_tuple(adj_label)\n",
    "    for epoch in range(FLAGS.epochs):# Train model   #train_loss, train_acc= [],[]\n",
    "        t = time.time()\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)    # Construct feed dictionary\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        # Run single weight update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.accuracy], feed_dict=feed_dict)\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        avg_accuracy = outs[2]\n",
    "        #    train_loss.append(avg_cost)\n",
    "        #   train_acc.append(avg_accuracy)\n",
    "        roc_curr, aupr_score = get_roc_score1(val_edges, val_edges_false)\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"train_acc=\", \"{:.5f}\".format(avg_accuracy), \"val_roc=\", \"{:.5f}\".format(val_roc_score[-1]),\n",
    "          \"val_ap=\", \"{:.5f}\".format(aupr_score),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    print(\"Optimization Finished!\")\n",
    "    x_train, x_test, x_val, y_train, y_test, y_val = node_trans_edges(test_edges[i],test_edges_false[i],val_edges[i],val_edges_false[i],train_edges[i],train_edges_false[i])\n",
    "    RocAuc = RocAucEvaluation(validation_data=(x_val,y_val), interval=1)\n",
    "    ####=====================deep learning model to predict =============================\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(256,)))\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(128,)))\n",
    "    model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0),   ##  optimizer='adam'\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])\n",
    "    print(\"model compile finished, model fit begin\")\n",
    "    model.fit(x_train,y_train, batch_size=50, epochs=200,validation_data=(x_val, y_val), callbacks=[RocAuc], verbose=2)\n",
    "    pre = model.predict(x_test)\n",
    "    pre_lab = np.argmax(model.predict(x_test), axis=1)\n",
    "    y_test = [y_test[i][1] for i in range(len(y_test))]\n",
    "    pre_probability = [pre[i][1] for i in range(len(pre))]\n",
    "    \n",
    "    roc_score,aupr_score,precision, recall,accuracy,f = get_roc_score2(pre_lab,pre_probability,y_test)\n",
    "    roc_score_arr.append(roc_score)\n",
    "    aupr_score_arr.append(aupr_score)\n",
    "    precision_arr.append(precision)\n",
    "    recall_arr.append(recall)\n",
    "    accuacy_arr.append(accuracy)\n",
    "    f_arr.append(f)\n",
    "    print(roc_score,aupr_score,precision, recall,accuracy,f)\n",
    "\n",
    "roc_score = np.mean(roc_score_arr)\n",
    "aupr_score = np.mean(aupr_score_arr)\n",
    "precision = np.mean(precision_arr)\n",
    "recall = np.mean(recall_arr)\n",
    "accuracy = np.mean(accuacy_arr)\n",
    "f = np.mean(f_arr)\n",
    "\n",
    "print( \"roc_score=\", \"{:.5f}\".format(roc_score), \"aupr_score =\", \"{:.5f}\".format(aupr_score ),\n",
    "          \"precision=\", \"{:.5f}\".format(precision),  \"recall=\", \"{:.5f}\".format(recall),\n",
    "            \"accuracy =\", \"{:.5f}\".format(accuracy ),  \"f =\", \"{:.5f}\".format(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results\n",
    "\n",
    "**TO DO**\n",
    "\n",
    "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
    "\n",
    "Please test and report results for all experiments that you run with:\n",
    "\n",
    "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
    "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "source": [
    "**TO DO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "**TO DO**\n",
    "\n",
    "In this section, you should discuss your work and make future plan. The discussion should address the following questions:\n",
    "  * Make assessment that the paper is reproducible or not.\n",
    "  * Explain why it is not reproducible if your results are kind negative.\n",
    "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
    "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
    "  * What will you do in next phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Feng, YH., Zhang, SW. & Shi, JY. DPDDI: a deep predictor for drug-drug interactions. BMC Bioinformatics 21, 419 (2020). https://doi.org/10.1186/s12859-020-03724-x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
